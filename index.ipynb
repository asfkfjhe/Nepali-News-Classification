{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nepali News Classification \n",
    "Compare the accuracy of different models using different algorithms like LSTM , GRU to classify the news dataset into different categories (politics, sport, entertainment, tech, business). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improrting Data and Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supriya/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/supriya/Desktop/FDV/Nepali-News-Classification/Nepali_Dataset_New.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4540, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n﻿काठमाण्डौ, ६ असार । नेपाल ललितकला प्रज्ञा प...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>नेपालको आशा जीवितैकप्तान पारस खड्काले ब्याट र ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n﻿राजविराज, २६ फागुन । नेकपा एमालेलाई औद्योगि...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>सामाजिक कार्यका लागि सुन्दरी\"\\nसौन्दर्य प्रतिय...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>खराब प्रदर्शनपछि प्रशिक्षण पिच\"महिला राष्ट्रिय...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News       Category\n",
       "0  \\n﻿काठमाण्डौ, ६ असार । नेपाल ललितकला प्रज्ञा प...       politics\n",
       "1  नेपालको आशा जीवितैकप्तान पारस खड्काले ब्याट र ...          sport\n",
       "2  \\n﻿राजविराज, २६ फागुन । नेकपा एमालेलाई औद्योगि...       politics\n",
       "3  सामाजिक कार्यका लागि सुन्दरी\"\\nसौन्दर्य प्रतिय...  entertainment\n",
       "4  खराब प्रदर्शनपछि प्रशिक्षण पिच\"महिला राष्ट्रिय...          sport"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politics', 'sport', 'entertainment', 'tech', 'business'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {\n",
    "    'politics' : 0, \n",
    "    'sport' : 1, \n",
    "    'entertainment' : 2, \n",
    "    'tech' : 3, \n",
    "    'business': 4\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning , Tokenizing and Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nepali_stemmer.stemmer import NepStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    #removing unnecessary symbols\n",
    "    clean_text = re.sub(r'[\\n,|।\\'\":]', '', text)\n",
    "\n",
    "\n",
    "    #Tokenizing Text \n",
    "    nepstem = NepStemmer()\n",
    "    tokenized_text = nepstem.stem(clean_text)\n",
    "\n",
    "    #removing stopwords\n",
    "    nep_stopwords = set(stopwords.words('nepali'))\n",
    "    words = tokenized_text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in nep_stopwords]\n",
    "    C_T_S = ' '.join(filtered_words)\n",
    "\n",
    "    return C_T_S\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "- Initial Splitting : X, y -> train_x, train_y \n",
    "- Tokenize train_x\n",
    "- Further splitting for validation and testing \n",
    "        -To prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "23 23 181\n",
      "CPU times: user 10.1 s, sys: 227 ms, total: 10.3 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset['News']\n",
    "y = dataset['Category']\n",
    "\n",
    "train_x, _, train_y, _ = train_test_split(x, y, stratify=y, test_size=0.95)\n",
    "\n",
    "# clean the text\n",
    "train_x = train_x.apply(lambda x: clean_and_tokenize(str(x)))\n",
    "\n",
    "# convert the categories to ids\n",
    "train_y = train_y.apply(lambda x: cats[x])\n",
    "\n",
    "# first split the train dataset into (train, rest) set\n",
    "train_x, rest_x, train_y, rest_y = train_test_split(train_x, train_y, stratify=train_y, test_size=0.2, random_state=747)\n",
    "\n",
    "# then split the rest into (test, valid) set\n",
    "test_x, valid_x, test_y, valid_y = train_test_split(rest_x, rest_y, stratify=rest_y, test_size=0.5, random_state=747)\n",
    "print(len(train_x))\n",
    "print(len(test_y), len(valid_y), len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframe\n",
    "- using the splitted data to create **train.csv** , **test.csv** , **valid.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_x, train_y], axis=1).reset_index()[['Category', 'News']]\n",
    "test_df = pd.concat([test_x, test_y], axis=1).reset_index()[['Category', 'News']]\n",
    "valid_df = pd.concat([valid_x, valid_y], axis=1).reset_index()[['Category', 'News']]\n",
    "\n",
    "train_df.to_csv('train.csv', header=['classlabel', 'content'], encoding='utf-8', index=False)\n",
    "test_df.to_csv('test.csv', header=['classlabel', 'content'], encoding='utf-8', index=False)\n",
    "valid_df.to_csv('valid.csv', header=['classlabel', 'content'], encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py      GRU.py       LSTM.py                 README.md     test.csv\n",
      "FastApi.py  index.ipynb  Nepali_Dataset_New.csv  RNN.pth       train.csv\n",
      "GRU.pth     index.py     \u001b[0m\u001b[01;34m__pycache__\u001b[0m/            test_0.ipynb  valid.csv\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>आईओसी पेट्रोलियम पदार्थ आपूर्ति घटायोभारतीय आय...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>पहाडी जिल्ला आपूर्ति बन्दजिविस ठेकेदार आयातित ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>﻿राजविराज २७ फागुन सप्तरी मलेठ गाउँ विकास समित...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>सफ्टवेयर बाट भविष्यवाणीभाग्य विश्वास उत्तर एस ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>सिस्नो ढिंडो खाने सम्पन्नराजधानी निजी स्कुल गर...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               News\n",
       "0         4  आईओसी पेट्रोलियम पदार्थ आपूर्ति घटायोभारतीय आय...\n",
       "1         4  पहाडी जिल्ला आपूर्ति बन्दजिविस ठेकेदार आयातित ...\n",
       "2         0  ﻿राजविराज २७ फागुन सप्तरी मलेठ गाउँ विकास समित...\n",
       "3         3  सफ्टवेयर बाट भविष्यवाणीभाग्य विश्वास उत्तर एस ...\n",
       "4         4  सिस्नो ढिंडो खाने सम्पन्नराजधानी निजी स्कुल गर..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Data preprocessing and preparing Pytorch dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 60_000\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "BIDIRECTIONAL = True\n",
    "HIDDEN_DIM = 200\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True,\n",
    "#                   tokenize='spacy',\n",
    "                  include_lengths=True) # necessary for packed_padded_sequence\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset \n",
    "- create dataset using **TabularDataset** from **torchtext** which allows loading data from CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('classlabel', LABEL), ('content', TEXT)]\n",
    "\n",
    "train_dataset = data.TabularDataset(\n",
    "    path=\"train.csv\", format='csv',\n",
    "    skip_header=True, fields=fields)\n",
    "\n",
    "test_dataset = data.TabularDataset(\n",
    "    path=\"test.csv\", format='csv',\n",
    "    skip_header=True, fields=fields)\n",
    "\n",
    "valid_dataset = data.TabularDataset(\n",
    "    path=\"valid.csv\", format='csv',\n",
    "    skip_header=True, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 181\n",
      "Num Valid: 23\n",
      "Num Valid: 23\n"
     ]
    }
   ],
   "source": [
    "print(f'Num Train: {len(train_dataset)}')\n",
    "print(f'Num Valid: {len(test_dataset)}')\n",
    "print(f'Num Valid: {len(valid_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5571\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_dataset, test_dataset, valid_dataset,\n",
    "                 min_freq=2)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
    "    sort_key=lambda x: len(x.content),\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([181, 32])\n",
      "Target vector size: torch.Size([32])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([701, 23])\n",
      "Target vector size: torch.Size([23])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([1034, 23])\n",
      "Target vector size: torch.Size([23])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.content[0].size()}')\n",
    "    print(f'Target vector size: {batch.classlabel.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.content[0].size()}')\n",
    "    print(f'Target vector size: {batch.classlabel.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.content[0].size()}')\n",
    "    print(f'Target vector size: {batch.classlabel.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, bidirectional, hidden_dim, num_layers, output_dim, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_layers, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(5571, 200, padding_idx=1)\n",
      "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc1): Linear(in_features=400, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, BIDIRECTIONAL, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "model = model.to(DEVICE)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate accuracy of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.content\n",
    "            if 0 in text_lengths:\n",
    "                continue\n",
    "            logits = model(text, text_lengths.to('cpu'))\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            num_examples += batch_data.classlabel.size(0)\n",
    "            correct_pred += (predicted_labels.long() == batch_data.classlabel.long()).sum()\n",
    "        return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 000/006 | Cost: 1.5908\n",
      "training accuracy: 22.10%\n",
      "valid accuracy: 21.74%\n",
      "Time elapsed: 4.38 min\n",
      "Epoch: 002/050 | Batch 000/006 | Cost: 1.6059\n",
      "training accuracy: 35.91%\n",
      "valid accuracy: 30.43%\n",
      "Time elapsed: 8.32 min\n",
      "Epoch: 003/050 | Batch 000/006 | Cost: 1.5287\n",
      "training accuracy: 55.80%\n",
      "valid accuracy: 34.78%\n",
      "Time elapsed: 12.16 min\n",
      "Epoch: 004/050 | Batch 000/006 | Cost: 1.4632\n",
      "training accuracy: 66.85%\n",
      "valid accuracy: 43.48%\n",
      "Time elapsed: 16.64 min\n",
      "Epoch: 005/050 | Batch 000/006 | Cost: 1.4139\n",
      "training accuracy: 72.38%\n",
      "valid accuracy: 43.48%\n",
      "Time elapsed: 20.90 min\n",
      "Epoch: 006/050 | Batch 000/006 | Cost: 1.3309\n",
      "training accuracy: 75.69%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 24.74 min\n",
      "Epoch: 007/050 | Batch 000/006 | Cost: 1.1706\n",
      "training accuracy: 72.93%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 28.59 min\n",
      "Epoch: 008/050 | Batch 000/006 | Cost: 1.2719\n",
      "training accuracy: 91.16%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 32.58 min\n",
      "Epoch: 009/050 | Batch 000/006 | Cost: 0.8465\n",
      "training accuracy: 86.19%\n",
      "valid accuracy: 43.48%\n",
      "Time elapsed: 36.20 min\n",
      "Epoch: 010/050 | Batch 000/006 | Cost: 0.8432\n",
      "training accuracy: 89.50%\n",
      "valid accuracy: 43.48%\n",
      "Time elapsed: 39.92 min\n",
      "Epoch: 011/050 | Batch 000/006 | Cost: 0.7904\n",
      "training accuracy: 94.48%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 43.45 min\n",
      "Epoch: 012/050 | Batch 000/006 | Cost: 0.5400\n",
      "training accuracy: 96.69%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 47.35 min\n",
      "Epoch: 013/050 | Batch 000/006 | Cost: 0.3673\n",
      "training accuracy: 97.24%\n",
      "valid accuracy: 65.22%\n",
      "Time elapsed: 51.05 min\n",
      "Epoch: 014/050 | Batch 000/006 | Cost: 0.3965\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 47.83%\n",
      "Time elapsed: 54.71 min\n",
      "Epoch: 015/050 | Batch 000/006 | Cost: 0.1699\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 47.83%\n",
      "Time elapsed: 58.40 min\n",
      "Epoch: 016/050 | Batch 000/006 | Cost: 0.1184\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 61.81 min\n",
      "Epoch: 017/050 | Batch 000/006 | Cost: 0.1193\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 65.17 min\n",
      "Epoch: 018/050 | Batch 000/006 | Cost: 0.0726\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 65.22%\n",
      "Time elapsed: 68.42 min\n",
      "Epoch: 019/050 | Batch 000/006 | Cost: 0.0756\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 71.79 min\n",
      "Epoch: 020/050 | Batch 000/006 | Cost: 0.1046\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 75.14 min\n",
      "Epoch: 021/050 | Batch 000/006 | Cost: 0.0466\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 60.87%\n",
      "Time elapsed: 78.47 min\n",
      "Epoch: 022/050 | Batch 000/006 | Cost: 0.2060\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 81.75 min\n",
      "Epoch: 023/050 | Batch 000/006 | Cost: 0.0305\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 85.06 min\n",
      "Epoch: 024/050 | Batch 000/006 | Cost: 0.0250\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 47.83%\n",
      "Time elapsed: 88.35 min\n",
      "Epoch: 025/050 | Batch 000/006 | Cost: 0.0302\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 91.84 min\n",
      "Epoch: 026/050 | Batch 000/006 | Cost: 0.0047\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 95.12 min\n",
      "Epoch: 027/050 | Batch 000/006 | Cost: 0.0704\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 60.87%\n",
      "Time elapsed: 98.39 min\n",
      "Epoch: 028/050 | Batch 000/006 | Cost: 0.0130\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 102.45 min\n",
      "Epoch: 029/050 | Batch 000/006 | Cost: 0.0291\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 60.87%\n",
      "Time elapsed: 106.20 min\n",
      "Epoch: 030/050 | Batch 000/006 | Cost: 0.0798\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 109.48 min\n",
      "Epoch: 031/050 | Batch 000/006 | Cost: 0.0112\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 112.61 min\n",
      "Epoch: 032/050 | Batch 000/006 | Cost: 0.0075\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 52.17%\n",
      "Time elapsed: 116.50 min\n",
      "Epoch: 033/050 | Batch 000/006 | Cost: 0.0249\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 120.60 min\n",
      "Epoch: 034/050 | Batch 000/006 | Cost: 0.0051\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 124.23 min\n",
      "Epoch: 035/050 | Batch 000/006 | Cost: 0.0053\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 127.74 min\n",
      "Epoch: 036/050 | Batch 000/006 | Cost: 0.0429\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 131.09 min\n",
      "Epoch: 037/050 | Batch 000/006 | Cost: 0.0031\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 134.78 min\n",
      "Epoch: 038/050 | Batch 000/006 | Cost: 0.0084\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 138.44 min\n",
      "Epoch: 039/050 | Batch 000/006 | Cost: 0.0096\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 142.09 min\n",
      "Epoch: 040/050 | Batch 000/006 | Cost: 0.0053\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 145.77 min\n",
      "Epoch: 041/050 | Batch 000/006 | Cost: 0.0080\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text, text_lengths = batch_data.content\n",
    "        # print(text.shape, text_lengths.shape)\n",
    "        if 0 in text_lengths:\n",
    "            continue\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_lengths.to('cpu'))\n",
    "        cost = F.cross_entropy(logits, batch_data.classlabel.long())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of LSTM \n",
    "\n",
    "After **50** epochs, seems like the model has achieved 100% accuracy on the training set, which suggests that the model might have **overfit** the training data\n",
    "- **Validation Accuracy : 65.22%**\n",
    "- **Test Accuracy : 60.87%**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {\n",
    "    'politics' : 0, \n",
    "    'sport' : 1, \n",
    "    'entertainment' : 2, \n",
    "    'tech' : 3, \n",
    "    'business': 4\n",
    "   \n",
    "}\n",
    "\n",
    "map_dict = {v: k for k, v in cats.items()}\n",
    "\n",
    "def predict(model, sentence, device='cpu'):\n",
    "    model.eval()\n",
    "    indexed = [TEXT.vocab.stoi[token] for token in clean_and_tokenize(sentence).split()]\n",
    "#     indexed = [TEXT.vocab.stoi[i] for i in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    \n",
    "    output = model(tensor, length_tensor)\n",
    "    predictions = torch.softmax(output, dim=1)\n",
    "    \n",
    "    probs, label = predictions.max(dim=1)\n",
    "    \n",
    "    return predictions, probs.item(), label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Label: 1 -> sport\n"
     ]
    }
   ],
   "source": [
    "news = \"\"\"\n",
    "काठमाडौं । अमेरिका र वेस्ट इन्डिजमा अर्को महिना हुने आईसीसी टी-ट्वेन्टी विश्वकप खेल्ने नेपाली राष्ट्रिय क्रिकेट टिमको बुधबार घोषणा भएको छ । नेपाल क्रिकेट संघ क्यानले नेपाली टिम आज सार्वजनिक गरेको हो ।\n",
    "रोहित पौडेलको कप्तानीमा मुख्य प्रशिक्षक मोन्टी देसाइले कमलसिंह ऐरी र सागर ढकाललाई समेट्दै १५ सदस्यीय टोली घोषणा गरे। विश्वकप खेल्ने टिममा अशिफ शेख, अनिल साह, कुशल भुर्तेल, कुशल मल्ल, दिपेन्द्रसिंह ऐरी, ललित राजवंशी, करण केसी, गुल्सन झा, सोमपाल कामी, प्रतिश जिसी, सन्दीप जोरा र अभिनास बोहरा छन् ।\n",
    "\"\"\"\n",
    "preds, probs, label = predict(model, news)\n",
    "\n",
    "print(f'Class Label: {label} -> {map_dict[label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb Cell 125\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y250sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m news \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y250sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mकाठमाडौँ । नेपालमा स्वदेशी र विदेशी गरेर वार्षिक डेढ सयदेखि दुई सय वटासम्म चलचित्र प्रदर्शनमा आउने गरेका छन् । प्रदर्शनमा ल्याउनु अघि सबै चलचित्रले केन्द्रीय चलचित्र जाँच समिति (सेन्सर बोर्ड) बाट प्रदर्शनका लागि खुला (सेन्सर पास) प्रमाणपत्र लिन अनिवार्य हुन्छ ।\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y250sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y250sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y250sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m preds, probs, label \u001b[39m=\u001b[39m predict(model, news)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y250sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mClass Label: \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m -> \u001b[39m\u001b[39m{\u001b[39;00mmap_dict[label]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "news = \"\"\"\n",
    "काठमाडौँ । नेपालमा स्वदेशी र विदेशी गरेर वार्षिक डेढ सयदेखि दुई सय वटासम्म चलचित्र प्रदर्शनमा आउने गरेका छन् । प्रदर्शनमा ल्याउनु अघि सबै चलचित्रले केन्द्रीय चलचित्र जाँच समिति (सेन्सर बोर्ड) बाट प्रदर्शनका लागि खुला (सेन्सर पास) प्रमाणपत्र लिन अनिवार्य हुन्छ ।\n",
    "\n",
    "\"\"\"\n",
    "preds, probs, label = predict(model, news)\n",
    "\n",
    "print(f'Class Label: {label} -> {map_dict[label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"RNN.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, bidirectional, hidden_dim, num_layers, output_dim, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_dim,\n",
    "                          num_layers=num_layers,\n",
    "                          bidirectional=bidirectional, \n",
    "                          dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_layers, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
    "        packed_output, hidden = self.gru(packed_embedded)\n",
    "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (embedding): Embedding(5202, 200, padding_idx=1)\n",
      "  (gru): GRU(200, 200, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc1): Linear(in_features=400, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model_1 = GRU(INPUT_DIM, EMBEDDING_DIM, BIDIRECTIONAL, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "model_1 = model_1.to(DEVICE)\n",
    "print(model_1)\n",
    "optimizer = torch.optim.Adam(model_1.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/030 | Batch 000/006 | Cost: 1.8624\n",
      "training accuracy: 35.36%\n",
      "valid accuracy: 30.43%\n",
      "Time elapsed: 2.57 min\n",
      "Epoch: 002/030 | Batch 000/006 | Cost: 1.6761\n",
      "training accuracy: 51.93%\n",
      "valid accuracy: 30.43%\n",
      "Time elapsed: 5.11 min\n",
      "Epoch: 003/030 | Batch 000/006 | Cost: 1.5015\n",
      "training accuracy: 70.72%\n",
      "valid accuracy: 39.13%\n",
      "Time elapsed: 7.64 min\n",
      "Epoch: 004/030 | Batch 000/006 | Cost: 1.4099\n",
      "training accuracy: 72.38%\n",
      "valid accuracy: 47.83%\n",
      "Time elapsed: 10.18 min\n",
      "Epoch: 005/030 | Batch 000/006 | Cost: 1.2767\n",
      "training accuracy: 80.66%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 12.73 min\n",
      "Epoch: 006/030 | Batch 000/006 | Cost: 1.1164\n",
      "training accuracy: 90.61%\n",
      "valid accuracy: 56.52%\n",
      "Time elapsed: 15.25 min\n",
      "Epoch: 007/030 | Batch 000/006 | Cost: 0.9552\n",
      "training accuracy: 91.71%\n",
      "valid accuracy: 60.87%\n",
      "Time elapsed: 17.77 min\n",
      "Epoch: 008/030 | Batch 000/006 | Cost: 0.7979\n",
      "training accuracy: 95.03%\n",
      "valid accuracy: 60.87%\n",
      "Time elapsed: 20.23 min\n",
      "Epoch: 009/030 | Batch 000/006 | Cost: 0.5574\n",
      "training accuracy: 98.34%\n",
      "valid accuracy: 69.57%\n",
      "Time elapsed: 22.64 min\n",
      "Epoch: 010/030 | Batch 000/006 | Cost: 0.4020\n",
      "training accuracy: 98.90%\n",
      "valid accuracy: 69.57%\n",
      "Time elapsed: 24.96 min\n",
      "Epoch: 011/030 | Batch 000/006 | Cost: 0.1412\n",
      "training accuracy: 98.90%\n",
      "valid accuracy: 73.91%\n",
      "Time elapsed: 27.18 min\n",
      "Epoch: 012/030 | Batch 000/006 | Cost: 0.4067\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 60.87%\n",
      "Time elapsed: 29.40 min\n",
      "Epoch: 013/030 | Batch 000/006 | Cost: 0.1703\n",
      "training accuracy: 99.45%\n",
      "valid accuracy: 69.57%\n",
      "Time elapsed: 31.47 min\n",
      "Epoch: 014/030 | Batch 000/006 | Cost: 0.1817\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 73.91%\n",
      "Time elapsed: 33.62 min\n",
      "Epoch: 015/030 | Batch 000/006 | Cost: 0.0650\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 78.26%\n",
      "Time elapsed: 35.78 min\n",
      "Epoch: 016/030 | Batch 000/006 | Cost: 0.0297\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 73.91%\n",
      "Time elapsed: 37.93 min\n",
      "Epoch: 017/030 | Batch 000/006 | Cost: 0.0790\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 73.91%\n",
      "Time elapsed: 40.08 min\n",
      "Epoch: 018/030 | Batch 000/006 | Cost: 0.0315\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 82.61%\n",
      "Time elapsed: 42.24 min\n",
      "Epoch: 019/030 | Batch 000/006 | Cost: 0.0563\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 78.26%\n",
      "Time elapsed: 44.41 min\n",
      "Epoch: 020/030 | Batch 000/006 | Cost: 0.0463\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 73.91%\n",
      "Time elapsed: 46.56 min\n",
      "Epoch: 021/030 | Batch 000/006 | Cost: 0.0334\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 78.26%\n",
      "Time elapsed: 48.28 min\n",
      "Epoch: 022/030 | Batch 000/006 | Cost: 0.0385\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 78.26%\n",
      "Time elapsed: 50.01 min\n",
      "Epoch: 023/030 | Batch 000/006 | Cost: 0.0243\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 73.91%\n",
      "Time elapsed: 52.10 min\n",
      "Epoch: 024/030 | Batch 000/006 | Cost: 0.0598\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 78.26%\n",
      "Time elapsed: 54.21 min\n",
      "Epoch: 025/030 | Batch 000/006 | Cost: 0.0248\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 73.91%\n",
      "Time elapsed: 56.32 min\n",
      "Epoch: 026/030 | Batch 000/006 | Cost: 0.0062\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 65.22%\n",
      "Time elapsed: 58.44 min\n",
      "Epoch: 027/030 | Batch 000/006 | Cost: 0.0492\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 65.22%\n",
      "Time elapsed: 59.98 min\n",
      "Epoch: 028/030 | Batch 000/006 | Cost: 0.0163\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 65.22%\n",
      "Time elapsed: 61.48 min\n",
      "Epoch: 029/030 | Batch 000/006 | Cost: 0.0273\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 65.22%\n",
      "Time elapsed: 63.63 min\n",
      "Epoch: 030/030 | Batch 000/006 | Cost: 0.2201\n",
      "training accuracy: 100.00%\n",
      "valid accuracy: 69.57%\n",
      "Time elapsed: 65.75 min\n",
      "Total Training Time: 65.75 min\n",
      "Test accuracy: 69.57%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_1.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text, text_lengths = batch_data.content\n",
    "        # print(text.shape, text_lengths.shape)\n",
    "        if 0 in text_lengths:\n",
    "            continue\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model_1(text, text_lengths.to('cpu'))\n",
    "        cost = F.cross_entropy(logits, batch_data.classlabel.long())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL_1 PARAMETERS\n",
    "        optimizer.step()\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model_1, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model_1, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model_1, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of GRU Model \n",
    "\n",
    "- **Validation Accuracy : 69.87%**\n",
    "- **Test Accuracy : 69.57%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU performance better than LSTM \n",
    "\n",
    "- **Simpler Architecture**: The GRU has a simpler architecture compared to LSTM, with fewer parameters. This simplicity may be the reason for GRU to be trained more efficiently and  prevent overfitting.\n",
    "- **Fewer Gating Mechanisms**: While both GRU and LSTM use gating mechanisms to control the flow of information, GRU has fewer gating mechanisms (update and reset gates) compared to LSTM (input, forget, and output gates). This reduced complexity may make it easier for the GRU to capture relevant information and avoid vanishing gradient problems.\n",
    "- **Dataset Characteristics**: The performance of different RNN architectures can vary depending on the characteristics of the dataset, such as the length of sequences, the presence of long-range dependencies, and the complexity of patterns. It's possible that the dataset used for text classification favors the characteristics of the GRU model, leading to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Label: 0 -> politics\n"
     ]
    }
   ],
   "source": [
    "news = \"\"\"\n",
    "काठमाडौं । अमेरिका र वेस्ट इन्डिजमा अर्को महिना हुने आईसीसी टी-ट्वेन्टी विश्वकप खेल्ने नेपाली राष्ट्रिय क्रिकेट टिमको बुधबार घोषणा भएको छ । नेपाल क्रिकेट संघ क्यानले नेपाली टिम आज सार्वजनिक गरेको हो ।\n",
    "रोहित पौडेलको कप्तानीमा मुख्य प्रशिक्षक मोन्टी देसाइले कमलसिंह ऐरी र सागर ढकाललाई समेट्दै १५ सदस्यीय टोली घोषणा गरे। विश्वकप खेल्ने टिममा अशिफ शेख, अनिल साह, कुशल भुर्तेल, कुशल मल्ल, दिपेन्द्रसिंह ऐरी, ललित राजवंशी, करण केसी, गुल्सन झा, सोमपाल कामी, प्रतिश जिसी, सन्दीप जोरा र अभिनास बोहरा छन् ।\n",
    "\"\"\"\n",
    "preds, probs, label = predict(model_1, news)\n",
    "\n",
    "print(f'Class Label: {label} -> {map_dict[label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Label: 2 -> entertainment\n"
     ]
    }
   ],
   "source": [
    "news = \"\"\"\n",
    "काठमाडौँ । नेपालमा स्वदेशी र विदेशी गरेर वार्षिक डेढ सयदेखि दुई सय वटासम्म चलचित्र प्रदर्शनमा आउने गरेका छन् । प्रदर्शनमा ल्याउनु अघि सबै चलचित्रले केन्द्रीय चलचित्र जाँच समिति (सेन्सर बोर्ड) बाट प्रदर्शनका लागि खुला (सेन्सर पास) प्रमाणपत्र लिन अनिवार्य हुन्छ ।\n",
    "\"\"\"\n",
    "preds, probs, label = predict(model_1, news)\n",
    "\n",
    "print(f'Class Label: {label} -> {map_dict[label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), \"GRU.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, \n",
    "                          hidden_dim, \n",
    "                          dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(hidden[-1,:,:])  # Take the last hidden state\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(5437, 100, padding_idx=1)\n",
      "  (rnn): RNN(100, 128, dropout=0.5)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supriya/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "\n",
    "TEXT.build_vocab(train_dataset, test_dataset, valid_dataset,\n",
    "                 min_freq=2)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "\n",
    "# Assuming you have defined TEXT.vocab and other necessary variables\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "EMBEDDING_DIM = 100  # Example value, please replace with your desired value\n",
    "HIDDEN_DIM = 128  # Example value, please replace with your desired value\n",
    "OUTPUT_DIM = 1  # Example value, please replace with your desired value\n",
    "DROPOUT = 0.5  # Example value, please replace with your desired value\n",
    "\n",
    "model_2 = SimpleRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "print(model_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {\n",
    "    'politics' : 0, \n",
    "    'sport' : 1, \n",
    "    'entertainment' : 2, \n",
    "    'tech' : 3, \n",
    "    'business': 4\n",
    "   \n",
    "}\n",
    "\n",
    "map_dict = {v: k for k, v in cats.items()}\n",
    "\n",
    "def predict(model, sentence, device='cpu'):\n",
    "    model.eval()\n",
    "    indexed = [TEXT.vocab.stoi[token] for token in clean_and_tokenize(sentence).split()]\n",
    "#     indexed = [TEXT.vocab.stoi[i] for i in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    \n",
    "    output = model(tensor, length_tensor)\n",
    "    predictions = torch.softmax(output, dim=1)\n",
    "    \n",
    "    probs, label = predictions.max(dim=1)\n",
    "    \n",
    "    return predictions, probs.item(), label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.content\n",
    "            if 0 in text_lengths:\n",
    "                continue\n",
    "            logits = model(text, text_lengths.to('cpu'))\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            num_examples += batch_data.classlabel.size(0)\n",
    "            correct_pred += (predicted_labels.long() == batch_data.classlabel.long()).sum()\n",
    "        return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 4 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb Cell 147\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y256sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m### FORWARD AND BACK PROP\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y256sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m logits \u001b[39m=\u001b[39m model_2(text, text_lengths\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y256sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cost \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, batch_data\u001b[39m.\u001b[39mclasslabel\u001b[39m.\u001b[39mlong())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y256sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/supriya/Desktop/FDV/Nepali-News-Classification/index.ipynb#Y256sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m cost\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mcross_entropy_loss(\u001b[39minput\u001b[39m, target, weight, _Reduction\u001b[39m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 4 is out of bounds."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_2.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text, text_lengths = batch_data.content\n",
    "        # print(text.shape, text_lengths.shape)\n",
    "        if 0 in text_lengths:\n",
    "            continue\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model_2(text, text_lengths.to('cpu'))\n",
    "        cost = F.cross_entropy(logits, batch_data.classlabel.long())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE model_2 PARAMETERS\n",
    "        optimizer.step()\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model_2, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model_2, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model_2, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
